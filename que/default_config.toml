verbose = false

[documents]
n_documents_per_query = 5
chunk_size = 50
chunk_step = 50
embedding_model = "paraphrase-multilingual-mpnet-base-v2"

[model]
model_id = "bartowski/gemma-2-9b-it-GGUF"
quant = "*Q5_K_M.gguf"


[prompts]
system_prompt = """Answer the user query based on the source documents or very well-known facts.

The source documents have the following format: 
"<|source|> : /path/to/source/file
...
"

Here are the source documents: {context}

End of sources.


You should provide your answer as a JSON blob, and also provide all relevant short source snippets from the documents on which you directly based your answer, and a confidence score as a float between 0 and 1. Do not use the file paths as snippets.

Your answer should be built as follows:

{{
  "answer": your_answer,
  "confidence_score": your_confidence_score,
  "found_an_answer": true | false,
  "source_snippets": {{
    "/path/to/snippet1/file": "snippet1",
    "/path/to/snippet2/file": "snippet2",
    ...
  }}
}}
<|endoftext|>

The source snippets should be very short, a few words at most, not whole sentences! And they MUST be extracted from the context, with the exact same wording and spelling, including the file name where they were found. 
If no source snippets are relevant, you should set "source_snippets" to `{{}}`

If your answer satisfies the user's query, set "found_an_answer" to "true". If it does not, set it to "false". 

Answer only with raw JSON.
You must end the JSON blob by using "<|endoftext|>".

Now begin!
"""

followup_prompt = """
We have the opportunity to further add to the user's queries with additional pieces of context.
You may recall or use previously provided pieces of context.
"{context}"
"""


context_template = """
<|source|> : {fname}
{snippet}
"""